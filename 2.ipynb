{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_photo_url(element, results):\n",
    "    aims = element.find_all('div',class_ = \"img_wrapper\" )\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'img':\n",
    "                results.append(child['src'])\n",
    "            \n",
    "\n",
    "# keywords\n",
    "def find_keywords(element, results):\n",
    "    aims = element.find_all('div',class_ = \"keywords\" )\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'a':\n",
    "                results.append(child.get_text())\n",
    "                           \n",
    "# content\n",
    "\n",
    "def find_body(element):\n",
    "    result = element.find_all('div',id = \"artibody\")\n",
    "    if len(result)==1:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return None            \n",
    "                        \n",
    "def find_content(element, results):\n",
    "    aims = element.find_all(name=['p', 'strong'])\n",
    "    for aim in aims:\n",
    "        if len(aim.find_all()) == 0:\n",
    "            results.append(aim.get_text().replace(\"\\u3000\",\"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").strip())\n",
    "\n",
    "# author\n",
    "\n",
    "def find_author(element, results):\n",
    "    aims = element.find_all('span',class_ = \"author\")\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'a':\n",
    "                results.append(child.get_text())\n",
    "\n",
    "# time\n",
    "def find_time_1(element, results):\n",
    "    aims = element.find_all('span',class_ = \"date\")          \n",
    "    for aim in aims:\n",
    "        results.append(aim.get_text())\n",
    "            \n",
    "def find_time_2(element, results):\n",
    "    aims = element.find_all('span',id = \"pub_date\")          \n",
    "    for aim in aims:\n",
    "        results.append(aim.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_templete ={\n",
    "        \"title\":\"\",\n",
    "        \"news_website\": \"\",\n",
    "        \"author\": \"\",\n",
    "        \"Managing Editor\":\"\",\n",
    "        \"creation_time\": \"\",\n",
    "        # \"comments_count\": \"\",\n",
    "        \"keywords\":[],\n",
    "        \"content\": [],\n",
    "        \"webpage\": \"\",\n",
    "        \"image_url\": []\n",
    "    }\n",
    "\n",
    "# start: helper function\n",
    "# image_url\n",
    "\n",
    "def find_photo_url(element, results):\n",
    "    aims = element.find_all('div',class_ = \"img_wrapper\" )\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'img':\n",
    "                results.append(child['src'])\n",
    "            \n",
    "\n",
    "# keywords\n",
    "def find_keywords(element, results):\n",
    "    aims = element.find_all('div',class_ = \"keywords\" )\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'a':\n",
    "                results.append(child.get_text())\n",
    "                           \n",
    "# content\n",
    "\n",
    "def find_body(element):\n",
    "    result = element.find_all('div',id = \"artibody\")\n",
    "    if len(result)==1:\n",
    "        return result[0]\n",
    "    else:\n",
    "        return None            \n",
    "                        \n",
    "def find_content(element, results):\n",
    "    aims = element.find_all(name=['p', 'strong'])\n",
    "    for aim in aims:\n",
    "        if len(aim.find_all()) == 0:\n",
    "            results.append(aim.get_text().replace(\"\\u3000\",\"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").strip())\n",
    "\n",
    "# author\n",
    "\n",
    "def find_author(element, results):\n",
    "    aims = element.find_all('span',class_ = \"author\")\n",
    "    for aim in aims:\n",
    "        for child in aim.children:\n",
    "            if child.name == 'a':\n",
    "                results.append(child.get_text())\n",
    "\n",
    "# time\n",
    "def find_time_1(element, results):\n",
    "    aims = element.find_all('span',class_ = \"date\")          \n",
    "    for aim in aims:\n",
    "        results.append(aim.get_text())\n",
    "            \n",
    "def find_time_2(element, results):\n",
    "    aims = element.find_all('span',id = \"pub_date\")          \n",
    "    for aim in aims:\n",
    "        results.append(aim.get_text())\n",
    "\n",
    "\n",
    "            \n",
    "# end: helper function---------\n",
    "\n",
    "# start: core function----------\n",
    "def get_news_data(url):\n",
    "    \n",
    "    article = {}\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    print(time_minus)\n",
    "    print(\"deepcopy\")\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BS(response.text, \"lxml\")\n",
    "    \n",
    "    \n",
    "    # web url\n",
    "    article[\"webpage\"] = url\n",
    "\n",
    "    # title\n",
    "    titles = soup.find_all('title')\n",
    "    if titles:\n",
    "        title = titles[0].string.strip().split(\"_\")\n",
    "        if len(title) >1:\n",
    "            article[\"title\"]=title[0].replace(\"|\",\"\")\n",
    "            article[\"news_website\"] =title[1].replace(\"|\",\"\")\n",
    "        else:\n",
    "            article[\"title\"]=title[0]\n",
    "    else:\n",
    "        print(f\"there is no title in the article from this url:{url},maybe something wrong\")\n",
    "    \n",
    "    \n",
    "    # image_url\n",
    "    photo_urls = []\n",
    "    find_photo_url(soup,photo_urls)\n",
    "    for photo_url in photo_urls:\n",
    "        article[\"image_url\"].append(\"https:\"+photo_url)\n",
    "    \n",
    "    \n",
    "    # content and Managing Editor\n",
    "    contents = []\n",
    "    body = find_body(soup)\n",
    "    if body:\n",
    "        find_content(body,contents)\n",
    "        for content in contents:\n",
    "            if \"责任编辑：\" in content:\n",
    "                editor_name = content.replace(\"责任编辑\",\" \").strip()\n",
    "                article[\"Managing Editor\"] = editor_name\n",
    "                contents.remove(content)\n",
    "            if \"文 |\" in content:\n",
    "                author_name = content.replace(\"文 |\",\" \").strip()\n",
    "                article[\"author\"] = author_name.replace(\"\\xa0\",\"\")\n",
    "                contents.remove(content)    \n",
    "        article[\"content\"] = contents\n",
    "    else:\n",
    "        print(\"cannot find the body\")\n",
    "    \n",
    "    \n",
    "    # author\n",
    "    if not article[\"author\"]:\n",
    "        authors = []\n",
    "        find_author(soup,authors)\n",
    "        if len(authors) != 0:\n",
    "            article[\"author\"] = authors[0]\n",
    "    time_minus = time.time()-start_time\n",
    "    start_time = time.time()\n",
    "    print(time_minus)\n",
    "    print(\"author\") \n",
    "    #time\n",
    "    times = []\n",
    "    find_time_1(soup,times)\n",
    "    if times:\n",
    "        article[\"creation_time\"] = times[0]\n",
    "    else:\n",
    "        find_time_2(soup,times)\n",
    "        if times:\n",
    "            article[\"creation_time\"] = times[0].replace(\"\\n\",\"\").strip()\n",
    "        else:\n",
    "            print(\"there is no time\")\n",
    "    \n",
    "    # keywords\n",
    "    keywords = []\n",
    "    find_keywords(soup,keywords)\n",
    "    article[\"keywords\"] = keywords\n",
    "    return article\n",
    "    \n",
    "# end: core function----------\n",
    "url = \"https://finance.sina.com.cn/tech/it/2023-05-04/doc-imysqtfv3409752.shtml\"\n",
    "get_news_data(url)\n",
    "\n",
    "num = 1\n",
    "flag = 0\n",
    "with open('url_tech.txt', 'r') as file:\n",
    "    article_list = []\n",
    "    for line in file:\n",
    "        if flag >= num:\n",
    "            break\n",
    "        else:\n",
    "            print(flag)\n",
    "        # 处理每一行的数据\n",
    "        url=line.strip()  # 输出处理后的行数据（去除行尾的换行符）\n",
    "        article_list.append(get_news_data(url))\n",
    "        flag =flag + 1\n",
    "\n",
    "with open(\"data1.json\", \"w\") as json_file:\n",
    "    json.dump(article_list, json_file, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
